"""Simple conversational chat interface using ConversationalStreamAdapter.

Phase 2: Clean, simple chat that leverages the new conversational streaming architecture
built in Phase 1. Starts with basic message streaming and builds from there.
"""

import uuid

import streamlit as st
from ai_engine.streaming.events import TokenStreamEvent, ToolCallEvent
from frontend.services.conversational_stream_adapter import (
    get_avatar,
    get_speaker_for_namespace,
    get_tool_status_display,
)
from frontend.services.stream_processor_integration import create_simple_conversational_processor
from langchain_core.messages import HumanMessage


def render_chat_interface():
    """Render the main conversational chat interface."""
    # Initialize session
    _init_chat_session()

    # Header
    _render_header()

    # Check if we have both a model and a graph
    has_model = "current_model" in st.session_state
    has_graph = "current_graph" in st.session_state

    if has_model and has_graph:
        st.markdown(
            f"<span style='font-size:0.95em;'>"
            f"ğŸŸ¢ <b>{st.session_state.current_provider}</b> &nbsp;|&nbsp; "
            f"ğŸ§  <b>{st.session_state.current_graph_info.name}</b> &nbsp;|&nbsp; "
            f"ğŸ§µ <code>{st.session_state.thread_id}</code>"
            f"</span>",
            unsafe_allow_html=True,
        )

        # Main chat area using conversational streaming
        _render_conversational_chat()

    elif not has_model:
        st.info("ğŸ‘ˆ Please configure a provider in the sidebar and test the connection to start chatting!")
    elif not has_graph:
        st.info("ğŸ‘ˆ Please select an AI agent in the sidebar to start chatting!")
    else:
        st.error("Unexpected state - please refresh the page")

    # Add debug section in development
    with st.expander("ğŸ”§ Development Debug Tools", expanded=False):
        _show_debug_info()

        if st.button("ğŸ§ª Add Test Messages"):
            _add_test_messages()

        st.subheader("Message Architecture V2 Info")
        st.info("""
        **Architecture V2: Sequential Message Flow**
        - Data Layer: ConversationalStreamAdapter updates st.session_state.messages
        - UI Layer: Chat Component renders messages chronologically  
        - Message types: user, ai, tool_call, artifact
        - Real-time: Adapter updates session state during streaming
        """)


def _render_header():
    """Render the header with clear conversation button."""
    col1, col2 = st.columns([4, 1])
    with col1:
        st.header("ğŸ’¬ Conversational AI Chat")
    with col2:
        if st.button("ğŸ—‘ï¸ Clear", help="Clear conversation history"):
            _clear_conversation()
            st.rerun()


def _render_conversational_chat():
    """Render conversation from st.session_state.messages in chronological order."""

    # Display all historical messages from session state
    for message in st.session_state.messages:
        if message["role"] == "user":
            _render_user_message(message)
        elif message["role"] == "ai":
            _render_ai_message(message)
        elif message["role"] == "tool_call":
            _render_tool_call(message)
        elif message["role"] == "artifact":
            _render_artifact_message(message)

    # Handle new user input
    if prompt := st.chat_input("What would you like to ask?"):
        # Add user message to session state
        # st.session_state.messages.append(
        #     {
        #         "role": "user",
        #         "content": prompt,
        #         "timestamp": str(uuid.uuid4()),  # Simple ID for user messages
        #     }
        # )

        # Display user message immediately
        with st.chat_message("user"):
            st.markdown(prompt)

        # Start streaming (adapter will update session state)
        _stream_conversational_response(prompt)


def _render_user_message(message):
    """Render user message."""
    with st.chat_message("user"):
        st.markdown(message["content"])


def _render_ai_message(message):
    """Render AI message with proper speaker identification."""
    # Determine speaker and avatar from namespace
    speaker = get_speaker_for_namespace(message.get("namespace", "main"))
    avatar = get_avatar(speaker)

    with st.chat_message("assistant", avatar=avatar):
        # Speaker identification (if not main AI)
        if speaker != "AI":
            st.caption(f"ğŸ¤– {speaker} !")

        # Message content
        st.markdown(message["content"])

        # Display artifacts if present
        if "artifacts" in message and message["artifacts"]:
            _render_inline_artifacts(message["artifacts"])


def _render_tool_call(message):
    """Render tool call as inline work indicator."""
    # Tool status display
    tool_display = get_tool_status_display(message)
    st.caption(tool_display)

    # Expandable result for completed tools with results
    if message["status"] == "result_success" and message.get("result"):
        result_content = message["result"]["content"]
        result_text = str(result_content)

        if len(result_text) > 100:  # Show in expander for long results
            with st.expander(f"{message['name']} : {result_text[:25]}...", expanded=False):
                if isinstance(result_content, dict) or isinstance(result_content, list):
                    st.json(result_content)
                else:
                    st.text(result_text)
        else:
            # Show short results inline
            st.caption(f"Result: {result_text}")

    # Show error details for failed tools
    elif message["status"] == "result_error" and message.get("result"):
        with st.expander(f"Error details for {message['name']}", expanded=False):
            st.error(str(message["result"]))


def _render_artifact_message(message):
    """Render standalone artifact message."""
    # Determine speaker from namespace
    speaker = get_speaker_for_namespace(message.get("namespace", "main"))
    avatar = get_avatar(speaker)

    with st.chat_message("assistant", avatar=avatar):
        if speaker != "AI":
            st.caption(f"ğŸ¤– {speaker}")

        st.info(f"ğŸ“‹ Created {message['artifact_type']}")
        # Could expand this to show artifact content based on type
        if st.button(f"View {message['artifact_type']}", key=f"artifact_{message.get('timestamp', 'unknown')}"):
            st.json(message["artifact_data"])


def _render_inline_artifacts(artifacts):
    """Render artifacts inline with AI message."""
    with st.expander(f"ğŸ“‹ {len(artifacts)} Artifact(s)", expanded=False):
        for i, artifact in enumerate(artifacts):
            st.subheader(f"ğŸ“‹ {artifact['type']}")
            st.json(artifact["data"])
            if i < len(artifacts) - 1:
                st.divider()


def _stream_conversational_response(user_input: str):
    """Simple real-time streaming focusing on the main experience."""
    try:
        # Get or create conversational processor
        if "conversational_processor" not in st.session_state:
            st.session_state.conversational_processor = create_simple_conversational_processor()

        processor = st.session_state.conversational_processor
        # processor.reset_session()

        # Prepare graph input
        input_state = {"messages": [HumanMessage(content=user_input)], "artifacts": [], "research_iteration": 0}
        config = {"configurable": {"thread_id": st.session_state.thread_id}}

        # Simple streaming generator focusing on readability
        async def simple_stream():
            """Simple streaming that prioritizes working over complexity."""

            async for event in processor.stream_with_conversation(st.session_state.current_graph, input_state, config):
                if isinstance(event, TokenStreamEvent) and hasattr(event, "content_delta") and event.content_delta:
                    yield event.content_delta
                elif isinstance(event, ToolCallEvent) and hasattr(event, "status") and hasattr(event, "tool_name"):
                    # Show key tool events inline
                    if event.status in ["args_started"]:
                        yield f"\n\nğŸ”§ *Calling {event.tool_name}...*\n\n"
                    elif event.status == "args_streaming":
                        yield f"{event.args_delta}"
                    elif event.status == "args_ready":
                        yield event.args
                    elif event.status == "result_success":
                        # st.json(event.args)
                        yield f"\n\nâœ… *{event.tool_name} completed*\n: {event.result.get('content', '') if event.result else ''}\n\n"

        # Use Streamlit's native streaming
        with st.chat_message("assistant", avatar="ğŸ¤–"):
            st.write_stream(simple_stream())
            st.rerun()

    except Exception as e:
        st.error(f"Error in simple streaming: {str(e)}")
        st.exception(e)


def _init_chat_session():
    """Initialize chat session state."""
    if "thread_id" not in st.session_state:
        st.session_state.thread_id = str(uuid.uuid4())
        st.session_state.user_id = "demo_user"

    if "messages" not in st.session_state:
        st.session_state.messages = []

    if "conversational_processor" not in st.session_state:
        st.session_state.conversational_processor = create_simple_conversational_processor()


def _clear_conversation():
    """Clear the conversation and reset session."""
    st.session_state.messages = []
    st.session_state.thread_id = str(uuid.uuid4())

    # Reset conversational processor
    if "conversational_processor" in st.session_state:
        st.session_state.conversational_processor.reset_session()


# Debugging utilities
def _show_debug_info():
    """Show debug information about current conversation state."""
    if st.button("ğŸ” Debug: Show Message State"):
        st.subheader("Current Message State")
        st.json(st.session_state.messages)

        if "conversational_processor" in st.session_state:
            adapter = st.session_state.conversational_processor.get_adapter()
            summary = adapter.get_conversation_summary()
            st.subheader("Conversation Summary")
            st.json(summary)


def _add_test_messages():
    """Add test messages to verify the rendering works correctly."""
    test_messages = [
        {
            "role": "user",
            "content": "Analyze this data and create a comprehensive report",
            "timestamp": "2024-01-01T10:00:00",
        },
        {
            "id": "msg_2",
            "namespace": "main",
            "role": "ai",
            "content": "I'll help you analyze the data and create a comprehensive report for you...",
            "timestamp": "2024-01-01T10:00:01",
        },
        {
            "namespace": "main",
            "message_id": "msg_2",
            "tool_call_id": "call_123",
            "role": "tool_call",
            "name": "think_tool",
            "status": "result_success",
            "args": {"reflection": "The user is asking me to analyze data..."},
            "result": "Reflection complete - proceeding with analysis",
            "timestamp": "2024-01-01T10:00:02",
        },
        {
            "namespace": "main",
            "message_id": "msg_2",
            "tool_call_id": "call_456",
            "role": "tool_call",
            "name": "analysis_agent",
            "status": "args_streaming",
            "args": None,
            "result": None,
            "timestamp": "2024-01-01T10:00:03",
        },
        {
            "id": "msg_3",
            "namespace": "analysis_agent:task_123",
            "role": "ai",
            "content": "Analyzing the data now... I've found several interesting patterns in the data. Analysis complete! Key findings: correlation coefficient 0.85, 3 main clusters identified.",
            "timestamp": "2024-01-01T10:00:04",
        },
        {
            "namespace": "analysis_agent:task_123",
            "message_id": "msg_3",
            "tool_call_id": "call_789",
            "role": "tool_call",
            "name": "data_processor",
            "status": "result_success",
            "args": {"query": "deep analysis"},
            "result": "covariance: 12.5, variance: 8.2, trends: upward",
            "timestamp": "2024-01-01T10:00:05",
        },
        {
            "id": "msg_4",
            "namespace": "main",
            "role": "ai",
            "content": "Here's your comprehensive analysis based on the detailed processing: The data shows strong correlation (0.85) with 3 distinct clusters and an upward trend. Covariance of 12.5 indicates significant relationships between variables.",
            "artifacts": [
                {
                    "type": "AnalysisReport",
                    "data": {"correlation": 0.85, "clusters": 3, "trend": "upward"},
                    "namespace": "main",
                    "timestamp": "2024-01-01T10:00:06",
                }
            ],
            "timestamp": "2024-01-01T10:00:06",
        },
    ]

    # Add test messages to session state
    st.session_state.messages.extend(test_messages)
    st.success(f"Added {len(test_messages)} test messages demonstrating the sequential conversation flow!")
    st.rerun()

    #
    #
    #
    #
    #
    #
    #
