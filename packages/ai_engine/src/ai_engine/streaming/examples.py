"""Real streaming examples using the new modular streaming system.

Demonstrates actual LangGraph execution with streaming events,
not simulated data. Uses the supervisor graph for concrete examples.
"""

import asyncio

from ai_engine.agents.aiops_deepsearch_agent.graphs.deepsearch_graph import get_deepsearch_graph
from ai_engine.agents.aiops_supervisor_agent.graphs.supervisor_graph import get_supervisor_graph
from ai_engine.agents.clarify_agent.graphs.clarify_graph import get_clarify_graph
from ai_engine.streaming.config import ChannelConfig, ChannelType, StreamMode, TokenStreamingConfig
from ai_engine.streaming.events import ArtifactEvent, MessageReceivedEvent, TokenStreamEvent, ToolCallEvent
from ai_engine.streaming.processor import ChannelStreamingProcessor
from langchain_core.messages import HumanMessage


async def real_supervisor_streaming_example():
    """Real supervisor graph streaming with the new modular system."""
    print("ğŸš€ Real Supervisor Graph Streaming Example")
    print("=" * 50)

    # Configure channels to monitor
    channels = [
        ChannelConfig(key="messages", stream_mode=StreamMode.VALUES_ONLY, parse_messages=True),
        ChannelConfig(key="artifacts", artifact_type="artifact", channel_type=ChannelType.ARTIFACT),
    ]

    # Configure token streaming with tool calls
    token_config = TokenStreamingConfig(
        enabled_namespaces={"all"},
        include_tool_calls=True,
        # enabled_namespaces={"main", "orchestrate", "orchestrator_tools"}, include_tool_calls=False
    )

    # Create processor
    processor = ChannelStreamingProcessor(
        channels=channels,
        token_streaming=token_config,
    )

    # Get real supervisor graph
    # graph = get_supervisor_graph(name="streaming_supervisor_demo")
    # graph = get_deepsearch_graph(name="streaming_deepsearch_demo", include_clarify=False)
    graph = get_clarify_graph(name="streaming_clarify_demo")

    # Real input
    input_data = {
        "messages": [
            HumanMessage(
                content="What's going on with my App Langgraph Platform? it lags. Call both of you agents at the same time!"
            )
        ]
    }

    config = {"configurable": {"thread_id": "supervisor_streaming_demo"}}
    context = {"user_id": "demo_user"}

    print("ğŸ” Starting real supervisor execution with streaming...")
    print(f"ğŸ“Š Monitoring channels: {[c.key for c in channels]}")
    print(f"ğŸ¯ Token streaming from: {token_config.enabled_namespaces}")
    print(f"ğŸ”§ Tool calls enabled: {token_config.include_tool_calls}")
    print()

    # Track streaming statistics
    token_count = 0
    channel_updates = 0
    artifacts_created = 0
    tool_calls_seen = 0
    message_received_count = 0

    # Stream the real execution
    async for event in processor.stream(graph, input_data, config=config, context=context):
        if isinstance(event, TokenStreamEvent):
            token_count += 1
            if event.content_delta.strip():  # Only show non-empty tokens
                print(f"{event.content_delta}", end="| ", flush=True)

        elif isinstance(event, MessageReceivedEvent):
            message_received_count += 1
            print(f"\nğŸ’¬ [{event.namespace}] {event.message_type}: {event.message.content[:100]}...")

        elif isinstance(event, ArtifactEvent):
            artifacts_created += 1
            action = "Updated" if event.is_update else "Created"
            print(f"\nğŸ“„ [{event.namespace}] {action} {event.artifact_type}")
            print(f"    From channel '{event.channel}': {str(event.artifact_data)[:80]}...")

        elif isinstance(event, ToolCallEvent):
            if event.status == "args_started":
                tool_calls_seen += 1
                print(f"\nğŸ”§ [{event.namespace}] Started Tool call #{tool_calls_seen}: {event.tool_name}")
            if event.status == "args_streaming":
                if event.args_delta.strip():
                    print(f"\n âš™ï¸ [{event.namespace}] Tool args: {event.args_delta[:50]}...")
            if event.status == "args_ready":
                print(f"ğŸ—ï¸ Tool Call Construction Complete \n[{event.namespace}] Tool completed: {event.tool_name}")
                print(f"    Args: {event.args}")
            if event.status == "result_success":
                print(f"\nâœ… [{event.namespace}] Tool result: {event.result}")
            if event.status == "result_error":
                print(f"\nâŒ [{event.namespace}] Tool error: {event.error}")

    print(f"\n\nğŸ“ˆ Streaming Summary:")
    print(f"   ğŸ¯ Tokens streamed: {token_count}")
    print(f"   ğŸ“Š Channel updates: {channel_updates}")
    print(f"   ğŸ“„ Artifacts created: {artifacts_created}")
    print(f"   ğŸ”§ Tool calls: {tool_calls_seen}")
    print(f"   ğŸ’¬ Message received: {message_received_count}")
    print("âœ… Real supervisor streaming completed!")


async def message_deduplication_demo():
    """Demonstrate message parsing and deduplication."""
    print("ğŸ¯ Message Deduplication Demo")
    print("=" * 40)

    # Configure with both token streaming AND message parsing
    channels = [
        ChannelConfig(key="messages", parse_messages=True),  # Parse messages with deduplication
    ]

    token_config = TokenStreamingConfig(
        enabled_namespaces={"main"},  # Enable token streaming
        include_tool_calls=False,
    )

    processor = ChannelStreamingProcessor(
        channels=channels,
        token_streaming=token_config,
    )

    graph = get_supervisor_graph(name="dedup_demo")

    input_data = {"messages": [HumanMessage(content="Quick test for deduplication")]}

    print("ğŸ” This demo shows:")
    print("  1. Messages streamed token-by-token â†’ TokenStreamEvent")
    print("  2. Same messages in channels â†’ MessageReceivedEvent(was_streamed=True)")
    print("  3. ToolMessages only in channels â†’ MessageReceivedEvent(was_streamed=False)")
    print()

    token_events = 0
    message_events = 0

    async for event in processor.stream(graph, input_data):
        if isinstance(event, TokenStreamEvent):
            token_events += 1

            print(f"'{event.content_delta}", end="| ")

        elif isinstance(event, MessageReceivedEvent):
            message_events += 1
            status = "ğŸ”„ DUPLICATE" if event.was_streamed else "ğŸ†• NEW"
            print(
                f"\nğŸ’¬ {status} {event.message_type}: {event.message.content[:50]}... (id: {getattr(event.message, 'id', None)})"
            )
    print(processor._seen_message_ids)
    print(f"\nğŸ“Š Result: {token_events} token events, {message_events} message events")
    print("âœ… Deduplication demo completed!")


async def main():
    """Run all real streaming examples."""
    print("ğŸš€ Real Streaming Examples with Supervisor Graph")
    print("=" * 55)

    try:
        # Load environment variables for the supervisor graph
        from dotenv import load_dotenv

        load_dotenv()

        # Run examples
        await real_supervisor_streaming_example()
        # await message_deduplication_demo()

        print("\n" + "=" * 55)
        print("âœ… All real streaming examples completed successfully!")

    except ImportError as e:
        print(f"âš ï¸  Missing dependency: {e}")
        print("Make sure to install python-dotenv: pip install python-dotenv")
    except Exception as e:
        print(f"\nâŒ Example failed: {e}")
        import traceback

        traceback.print_exc()
        raise


if __name__ == "__main__":
    asyncio.run(main())
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
    ###
